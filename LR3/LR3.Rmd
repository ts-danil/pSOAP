---
title: "Лабораторная работа №3"
author: "Цопанов Данил"
date: "05/05/2020"
output: html_document
---

# Исследование возможностей автоматизации сбора данных о доменах
## Цель работы:
Собрать информацию о топ 15 доменах в категории "Multimedia" (Music and Audio).  

### Собираемые данные:  
  1. Домен
  2. IP
  3. IP Netblock
  3. Страна, город
  4. Адрес
  5. Телефон
  6. Хостинг (при наличии)
  7. Открытые порты
  8. Используемые web-технологии на сайте

## Исходные данные: 
Список доменов категории "Multimedia" (Music and Audio).  

### Конфигурация аппаратного обеспечения и общесистемного ПО:  
Процессор: 4 × Intel® Core™ i5-3230M CPU @ 2.60GHz  
Память: 7,89 Гб ОЗУ  
ОС: Windows 10 x64   

### Используемое ПО:  
  1.Rstudio IDE  
  2.nmap  
  3.dig  
  4.whois  
  5.whatweb  

## Варианты решения задачи:  
1. Собрать информацию вручную с помощью веб-браузера, инструментов whois, dig, nmap и т.д.  
2.Использоавть интегрированные инструменты такие как SpiderFoot, Maltego CE, Datasploit, Recon-ng.  
3.Самостоятельно разработать (для образовательных целей) автоматизированное решение для сбора информации.  

В данной работе используется третий вариант решения задачи.  

### Содержание лабораторной работы:
```{r cache=TRUE}
library(tidyverse)
get_sum_df <- function(company_url) {
  country_state <- NA
  dig <- system2('dig', company_url, stdout = TRUE)
  ip <- dig %>%
    grep(pattern = company_url, value = TRUE) %>%
    str_extract(pattern = "\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b")
  ip <- ip[!is.na(ip)]
 
  whois <- system2('whois', ip[1], stdout = TRUE)
  phones <- whois %>%
    grep(pattern = "Phone", value = TRUE, ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ") %>%
    data.table::transpose() %>%
    .[[2]] %>%
    unique() %>%
    str_c(collapse = " ")
 
  netblock <- whois %>%
    grep(pattern = "CIDR", value = TRUE, ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ", simplify = TRUE) %>%
    .[-1] %>%
    str_c(collapse = " ")
 
  country <- whois %>%
    grep(pattern = "Country",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ", simplify = TRUE) %>%
    .[-1]
 
  country_state <- whois %>%
    grep(pattern = "State",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ", simplify = TRUE) %>%
    .[-1]
  if(length(country_state)==0) country_state <- NA
 
  address <- whois %>%
    grep(pattern = "address",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ", simplify = TRUE) %>%
    .[-1] %>%
    str_c(collapse = " ")
 
  hosting <- whois %>%
    grep(pattern = "Hosting",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ")
  hosting <- lapply(hosting, collapse = " ", str_c) %>%
    str_c(collapse = " ")
 
  nmap <-
    system2('nmap',
            args = c('-p', '22,21,80,443', ip[1]),
            stdout = TRUE)
  ports <- nmap %>%
    grep(pattern = "open",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ") %>%
    data.table::transpose() %>%
    .[[1]] %>%
    str_c(collapse = " ")
  ip <- str_c(ip,collapse = ' ')
 
  company_sum <-
    data.frame(
      csum = c(
        company_url,
        ip,
        netblock,
        country,
        country_state,
        address,
        phones,
        hosting,
        ports
      ),
      row.names = c(
        'company_url',
        'ip',
        'netblock',
        'country',
        'country_state',
        'address',
        'phones',
        'hosting',
        'ports'
      )
    )
  company_sum
 
}
 
urls <- c("Last.fm", "Videolan.org", "Ableton.com", "Nch.com.au", "Steinberg.net", "Waves.com", "Ffmpeg.org", "Izotope.com", "Smartmusic.com", "Shoutcast.com", "Real.com","Winamp.com", "Mpc-hc.org", "Mixedinkey.com", "Fabfilter.com")
 
dfs <- lapply(urls, get_sum_df) 
result <- bind_cols(dfs) 
 
row.names(result) <- c('company_url',
        'ip',
        'netblock',
        'country',
        'country_state',
        'address',
        'phones',
        'hosting',
        'ports'
      )
colnames(result) <- map(result[1,],as.character) %>% unlist()
result <- result[-1,]
knitr::kable(result)
```

Отдельно соберем информацию о веб-технологиях, так как rappalyzer использует непосредственно формат rappalyzer (до этого использовали DataFrame) и самостоятельно строит таблицы.

```{r cache=TRUE}
library(rappalyzer)
rappalyze("Last.fm")
rappalyze("Videolan.org")
rappalyze("Ableton.com")
rappalyze("Nch.com.au")
rappalyze("Steinberg.net")
rappalyze("Waves.com")
rappalyze("Ffmpeg.org")
rappalyze("Izotope.com")
rappalyze("Smartmusic.com")
rappalyze("Shoutcast.com")
rappalyze("Real.com")
rappalyze("Winamp.com")
rappalyze("Mpc-hc.org")
rappalyze("Mixedinkey.com")
rappalyze("Fabfilter.com")
```

### Оценка результата.
В результате выполнения лабораторной работы получилось освоить навыки автоматизации аналитической работы. Удалось быстро собрать необходимую информацию по списку доменов. Я в восторге от полученного результата!    

### Вывод.    
Теперь подведем итог, третья лабораторная работа была очень интересной, мы научились автоматизированно собирать информацию о доменах. Команды вполняются автоматически, таблица заполняется, все само! Это ли не чудо? 